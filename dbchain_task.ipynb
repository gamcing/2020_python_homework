{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.5/dist-packages (2.5.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (from transformers) (1.14.5)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.5/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.9.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.5/dist-packages (from transformers) (1.12.16)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.5/dist-packages (from transformers) (2020.2.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.5/dist-packages (from transformers) (4.43.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.5/dist-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.5/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.5/dist-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.5/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.16 in /usr/local/lib/python3.5/dist-packages (from boto3->transformers) (1.15.16)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.5/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from sacremoses->transformers) (1.11.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.5/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.5/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.5/dist-packages (from botocore<1.16.0,>=1.15.16->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.5/dist-packages (from botocore<1.16.0,>=1.15.16->boto3->transformers) (2.7.3)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /usr/local/lib/python3.5/dist-packages (from botocore<1.16.0,>=1.15.16->boto3->transformers) (1.25.8)\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (1.14.5)\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.5/dist-packages (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.14.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas) (2018.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.5/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.5/dist-packages (from sklearn) (0.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0307 20:42:54.576982 140510250534656 file_utils.py:41] PyTorch version 1.4.0 available.\n",
      "I0307 20:43:00.559194 140510250534656 tokenization_utils.py:417] Model name './embeddings' not found in model shortcut name list (bert-base-dutch-cased, bert-base-finnish-uncased-v1, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-german-cased, bert-large-uncased-whole-word-masking-finetuned-squad, bert-base-cased, bert-base-cased-finetuned-mrpc, bert-base-finnish-cased-v1, bert-base-chinese, bert-large-cased-whole-word-masking-finetuned-squad, bert-large-uncased-whole-word-masking, bert-large-cased, bert-base-multilingual-cased, bert-base-uncased, bert-base-multilingual-uncased, bert-large-uncased, bert-large-cased-whole-word-masking). Assuming './embeddings' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0307 20:43:00.560566 140510250534656 tokenization_utils.py:446] Didn't find file ./embeddings/added_tokens.json. We won't load it.\n",
      "I0307 20:43:00.561405 140510250534656 tokenization_utils.py:446] Didn't find file ./embeddings/tokenizer_config.json. We won't load it.\n",
      "I0307 20:43:00.562138 140510250534656 tokenization_utils.py:446] Didn't find file ./embeddings/special_tokens_map.json. We won't load it.\n",
      "I0307 20:43:00.562930 140510250534656 tokenization_utils.py:499] loading file None\n",
      "I0307 20:43:00.563646 140510250534656 tokenization_utils.py:499] loading file ./embeddings/vocab.txt\n",
      "I0307 20:43:00.564338 140510250534656 tokenization_utils.py:499] loading file None\n",
      "I0307 20:43:00.565037 140510250534656 tokenization_utils.py:499] loading file None\n",
      "I0307 20:43:01.507119 140510250534656 tokenization_utils.py:417] Model name './embeddings' not found in model shortcut name list (bert-base-dutch-cased, bert-base-finnish-uncased-v1, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-german-cased, bert-large-uncased-whole-word-masking-finetuned-squad, bert-base-cased, bert-base-cased-finetuned-mrpc, bert-base-finnish-cased-v1, bert-base-chinese, bert-large-cased-whole-word-masking-finetuned-squad, bert-large-uncased-whole-word-masking, bert-large-cased, bert-base-multilingual-cased, bert-base-uncased, bert-base-multilingual-uncased, bert-large-uncased, bert-large-cased-whole-word-masking). Assuming './embeddings' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0307 20:43:01.508918 140510250534656 tokenization_utils.py:446] Didn't find file ./embeddings/added_tokens.json. We won't load it.\n",
      "I0307 20:43:01.510049 140510250534656 tokenization_utils.py:446] Didn't find file ./embeddings/tokenizer_config.json. We won't load it.\n",
      "I0307 20:43:01.510667 140510250534656 tokenization_utils.py:446] Didn't find file ./embeddings/special_tokens_map.json. We won't load it.\n",
      "I0307 20:43:01.511390 140510250534656 tokenization_utils.py:499] loading file None\n",
      "I0307 20:43:01.512080 140510250534656 tokenization_utils.py:499] loading file ./embeddings/vocab.txt\n",
      "I0307 20:43:01.512748 140510250534656 tokenization_utils.py:499] loading file None\n",
      "I0307 20:43:01.513433 140510250534656 tokenization_utils.py:499] loading file None\n",
      "I0307 20:43:01.537451 140510250534656 tokenization_utils.py:417] Model name './embeddings' not found in model shortcut name list (bert-base-dutch-cased, bert-base-finnish-uncased-v1, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-german-cased, bert-large-uncased-whole-word-masking-finetuned-squad, bert-base-cased, bert-base-cased-finetuned-mrpc, bert-base-finnish-cased-v1, bert-base-chinese, bert-large-cased-whole-word-masking-finetuned-squad, bert-large-uncased-whole-word-masking, bert-large-cased, bert-base-multilingual-cased, bert-base-uncased, bert-base-multilingual-uncased, bert-large-uncased, bert-large-cased-whole-word-masking). Assuming './embeddings' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0307 20:43:01.539759 140510250534656 tokenization_utils.py:446] Didn't find file ./embeddings/added_tokens.json. We won't load it.\n",
      "I0307 20:43:01.540422 140510250534656 tokenization_utils.py:446] Didn't find file ./embeddings/tokenizer_config.json. We won't load it.\n",
      "I0307 20:43:01.540946 140510250534656 tokenization_utils.py:446] Didn't find file ./embeddings/special_tokens_map.json. We won't load it.\n",
      "I0307 20:43:01.541574 140510250534656 tokenization_utils.py:499] loading file None\n",
      "I0307 20:43:01.542126 140510250534656 tokenization_utils.py:499] loading file ./embeddings/vocab.txt\n",
      "I0307 20:43:01.542732 140510250534656 tokenization_utils.py:499] loading file None\n",
      "I0307 20:43:01.543340 140510250534656 tokenization_utils.py:499] loading file None\n",
      "I0307 20:43:02.464264 140510250534656 configuration_utils.py:254] loading configuration file ./embeddings/config.json\n",
      "I0307 20:43:02.465921 140510250534656 configuration_utils.py:292] Model config BertConfig {\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0307 20:43:02.467536 140510250534656 modeling_utils.py:459] loading weights file ./embeddings/pytorch_model.bin\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current train loss: 1.0737128257751465\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from models import Baseline, BertPooling, BertGRUPooling\n",
    "from modules.losses import FocalLoss\n",
    "from tools.io import load_by_step\n",
    "from tools.metrics import AccuracyMetric, F1Metric\n",
    "from tools.train import EarlyStoping, Trainer\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_STEP = 3200\n",
    "VAL_PER_STEP = 200\n",
    "EPOCH = 10\n",
    "PAD_LENGTH = 140\n",
    "BERTPATH = './embeddings'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y=None):\n",
    "        super(TextDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.length = len(x)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(BERTPATH)\n",
    "        self.map = {'0':0,'1':1,'-1':2}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.y is not None:\n",
    "            y = torch.tensor(self.map[self.y[index]])\n",
    "            return self._process(self.x[index]), y\n",
    "        return self._process(self.x[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def _process(self, sentence):\n",
    "        tk_output = self.tokenizer.encode_plus(\n",
    "            sentence, max_length=PAD_LENGTH, pad_to_max_length=True\n",
    "        )\n",
    "        ids, seg, mask = list(map(lambda x:torch.LongTensor(x),[tk_output['input_ids'], \\\n",
    "            tk_output['token_type_ids'], tk_output['attention_mask']]))\n",
    "        return ids, seg, mask\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self):\n",
    "        super(MyTrainer, self).__init__()\n",
    "    \n",
    "    def train_batch(self, data):\n",
    "        (ids, seg, mask), y = data\n",
    "        ids, mask, seg, y = list(map(lambda x:x.to(device), [ids, mask, seg, y]))\n",
    "        y_pred = self.model(ids, mask, seg)\n",
    "        return y_pred, y\n",
    "    \n",
    "    def validate_batch(self, data):\n",
    "        (ids, seg, mask), y = data\n",
    "        ids, mask, seg, y = list(map(lambda x:x.to(device), [ids, mask, seg, y]))\n",
    "        y_pred = self.model(ids, mask, seg)\n",
    "        return y_pred, y\n",
    "    \n",
    "    def predict(self, test_loader):\n",
    "        self.model.load_state_dict(torch.load('./checkpoint/last_best.pt'))\n",
    "        with torch.no_grad():\n",
    "            y_preds = []\n",
    "            for ids, seg, mask in test_loader:\n",
    "                ids, mask, seg = list(map(lambda x:x.to(device), [ids, mask, seg]))\n",
    "                y_pred = self.model(ids, mask, seg)\n",
    "                y_preds += y_pred.argmax(dim=-1).cpu().numpy().tolist()\n",
    "            return y_preds\n",
    "\n",
    "def create_trainer():\n",
    "    model = BertGRUPooling(\n",
    "        bert_path=BERTPATH, hidden_dim=768, output_dim=3\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=2e-5\n",
    "    )\n",
    "    callbacks = [ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3),EarlyStoping(model, patience=4)]\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # criterion = FocalLoss(num_classes=3)\n",
    "    # criterion = LabelSmoothingLoss()\n",
    "    \n",
    "    trainer = MyTrainer()\n",
    "    trainer.build(model, optimizer, criterion, callbacks=callbacks)\n",
    "    return trainer\n",
    "\n",
    "def load_data(path, labeled=True):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        output = f.readlines()\n",
    "    if not labeled:\n",
    "        return output\n",
    "    sentences = list(map(lambda x:x.split('LABELIS:')[0].replace('\\n',''), output))\n",
    "    targets = list(map(lambda x:x.split('LABELIS:')[1].replace('\\n',''), output))\n",
    "    return np.asarray(sentences), np.asarray(targets)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "metric = F1Metric(average='macro')\n",
    "\n",
    "train_x, train_y = load_data('./data/v1_data/train_labeled.txt', labeled=True)\n",
    "test_x = load_data('./data/v1_data/test.txt', labeled=False)\n",
    "test_dataset = TextDataset(test_x)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=233, shuffle=True)\n",
    "kfold_output = []\n",
    "for train_index, val_index in kfold.split(train_x, train_y):\n",
    "    tx, ty = train_x[train_index], train_y[train_index]\n",
    "    cvx, cvy = train_x[val_index], train_y[val_index]\n",
    "\n",
    "    train_dataset = TextDataset(x=tx, y=ty)\n",
    "    val_dataset = TextDataset(x=cvx, y=cvy)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    trainer = create_trainer()\n",
    "    \n",
    "    trainer.fit_by_step(\n",
    "        train_loader=train_loader, \n",
    "        validate_loader=val_loader, \n",
    "        steps=NUM_STEP,\n",
    "        validate_per_step=VAL_PER_STEP, \n",
    "        metric=metric\n",
    "    )\n",
    "    fold_y_pred = trainer.predict(test_loader)\n",
    "    kfold_output.append(fold_y_pred)\n",
    "\n",
    "pd.DataFrame(np.array(kfold_output).T).to_csv('output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch===1.4.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/47/69/7a1291b74a3af0043db9048606daeb8b57cd9dea90b9df740485f3843878/torch-1.4.0-cp35-cp35m-manylinux1_x86_64.whl (753.4 MB)\n",
      "\u001b[K     |################################| 753.4 MB 63.4 MB/s eta 0:00:014  |#                               | 35.4 MB 3.4 MB/s eta 0:03:33     |##                              | 52.5 MB 157 kB/s eta 1:13:58     |#######################         | 564.0 MB 74.0 MB/s eta 0:00:03     |########################        | 578.9 MB 35.5 MB/s eta 0:00:05     |########################        | 586.7 MB 55.2 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting torchvision===0.5.0\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d6/d0/64a337d3fadf7bee37c699f8574b038b8b0207f5815a322eecd52c65417a/torchvision-0.5.0-cp35-cp35m-manylinux1_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |################################| 4.0 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (from torch===1.4.0) (1.14.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.5/dist-packages (from torchvision===0.5.0) (5.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from torchvision===0.5.0) (1.11.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.1.0\n",
      "    Uninstalling torch-1.1.0:\n",
      "      Successfully uninstalled torch-1.1.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.3.0\n",
      "    Uninstalling torchvision-0.3.0:\n",
      "      Successfully uninstalled torchvision-0.3.0\n",
      "Successfully installed torch-1.4.0 torchvision-0.5.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
